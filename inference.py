"""inference.py
åŠŸèƒ½å‡çº§ç‰ˆï¼š
1. æ”¯æŒ --only_eval å‚æ•°ï¼šä»…è¯»å– OOF è®¡ç®—æœ€ä½³é˜ˆå€¼å¹¶æ‰“å°ï¼Œä¸è¿›è¡Œåç»­æ¨ç†ï¼ˆç§’çº§åé¦ˆï¼‰ã€‚
2. æ¨ç†ä½¿ç”¨ä¸¥æ ¼ 2x TTAï¼šä»…åŸå›¾ + æ°´å¹³ç¿»è½¬ï¼ˆä¸¥ç¦ 90Â° æ—‹è½¬ï¼‰ã€‚
"""

import os
import argparse
import torch
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader
from tqdm import tqdm
import glob
from datetime import datetime

# åœ¨å¯¼å…¥ config ä¹‹å‰å…³é—­ç›®å½•åˆå§‹åŒ–å¼€å…³
os.environ['CONFIG_INIT_DIRS'] = '0'
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
import config
from model import get_model
from dataset import MedicalDataset

import albumentations as A
from albumentations.pytorch import ToTensorV2

def parse_args():
    parser = argparse.ArgumentParser(description="Final Submission Inference Script")
    
    # æ¥æ”¶æ¨¡å‹è·¯å¾„
    parser.add_argument('--model_paths', nargs='+', required=True, 
                        help="Path to model files. Use quotes for wildcards.")

    # OOF æ–‡ä»¶è·¯å¾„ (é»˜è®¤è¯»å– output/oof/oof_fold_*.csv)
    parser.add_argument('--oof_paths', nargs='+', default=[],
                        help="OOF csv paths (supports wildcards).")
    
    # [å…³é”®] ä»…è¯„æµ‹æ¨¡å¼å¼€å…³
    parser.add_argument('--only_eval', action='store_true', 
                        help="If set, only search best threshold based on OOF and exit.")
                        
    return parser.parse_args()


def build_test_transform():
    return A.Compose([
        A.Normalize(mean=config.IMG_MEAN, std=config.IMG_STD),
        ToTensorV2(),
    ])


def load_oof(oof_paths):
    """è¯»å–å¹¶åˆå¹¶ OOF CSV"""
    probs = []
    targets = []
    if not oof_paths:
        return np.array([]), np.array([])
        
    for p in oof_paths:
        df = pd.read_csv(p)
        if 'Preds' not in df.columns or 'Targets' not in df.columns:
            raise ValueError(f"OOF æ–‡ä»¶åˆ—åå¿…é¡»åŒ…å« Preds/Targetsï¼Œä½†åœ¨ {p} ä¸­æœªæ‰¾åˆ°")
        probs.append(df['Preds'].values.astype(np.float32))
        targets.append(df['Targets'].values.astype(np.int64))
    
    if not probs:
        return np.array([]), np.array([])
        
    probs = np.concatenate(probs, axis=0)
    targets = np.concatenate(targets, axis=0)
    return probs, targets


def search_best_threshold(probs: np.ndarray, targets: np.ndarray) -> float:
    """åœ¨ [0.2, 0.8] æœç´¢æœ€ä½³é˜ˆå€¼"""
    thresholds = np.linspace(0.2, 0.8, 601, dtype=np.float32)
    best_t = 0.5
    best_acc = -1.0
    
    # ç®€å•çš„å‘é‡åŒ–è®¡ç®—åŠ é€Ÿ
    for t in thresholds:
        preds = (probs >= t).astype(np.int64)
        acc = (preds == targets).mean()
        if acc > best_acc or (acc == best_acc and t < best_t):
            best_acc = acc
            best_t = float(t)
    
    print("\n" + "="*45)
    print(f" ğŸ“Š [OOF Evaluation] å†…éƒ¨éªŒè¯é›†è¯„æµ‹æŠ¥å‘Š")
    print(f" ---------------------------------------------")
    print(f" æ ·æœ¬æ€»æ•°: {len(targets)}")
    print(f" æœ€ä½³é˜ˆå€¼: {best_t:.4f}")
    print(f" æœ€ä½³ Acc: {best_acc:.6f}  (è¿™æ˜¯é¢„æœŸçš„ä¸Šçº¿)")
    print("="*45 + "\n")
    return best_t


def tta_2x(images: torch.Tensor):
    """ä¸¥æ ¼ 2x TTAï¼šä»…åŸå›¾ + æ°´å¹³ç¿»è½¬ï¼ˆä¸åšä»»ä½•æ—‹è½¬ï¼‰"""
    return [images, torch.flip(images, dims=[3])]

def main():
    args = parse_args()
    
    # --- 1. ç¡®å®š OOF æ–‡ä»¶ ---
    oof_files = []
    if args.oof_paths:
        for p in args.oof_paths:
            oof_files.extend(glob.glob(p))
    else:
        # é»˜è®¤å» config ç›®å½•æ‰¾
        oof_files.extend(glob.glob(os.path.join(config.OOF_DIR, 'oof_fold_*.csv')))

    oof_files = sorted(oof_files)
    if not oof_files:
        print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ° OOF æ–‡ä»¶ï¼Œå°†ä½¿ç”¨é»˜è®¤é˜ˆå€¼ 0.5ã€‚è¯·æ£€æŸ¥ {config.OOF_DIR}")
        best_threshold = 0.5
    else:
        print(f"[Info] åŠ è½½ {len(oof_files)} ä¸ª OOF æ–‡ä»¶è¿›è¡Œé˜ˆå€¼æœç´¢...")
        probs, targets = load_oof(oof_files)
        if len(targets) == 0:
             print("âš ï¸  OOFæ–‡ä»¶ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤é˜ˆå€¼0.5")
             best_threshold = 0.5
        else:
             best_threshold = search_best_threshold(probs, targets)

    # === [æ ¸å¿ƒé€»è¾‘] å¦‚æœåªæ˜¯è¯„æµ‹ï¼Œåˆ°è¿™é‡Œå°±ç»“æŸ ===
    if args.only_eval:
        print("âœ… è¯„æµ‹å®Œæˆ (--only_eval)ã€‚ä¸æ‰§è¡Œæ¨ç†æäº¤ã€‚")
        return

    # --- 2. æ­£å¼æ¨ç†æµç¨‹ ---
    model_files = []
    for path_pattern in args.model_paths:
        model_files.extend(glob.glob(path_pattern))

    if not model_files:
        print(f"âŒ Error: No model files found matching: {args.model_paths}")
        return
    
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ¨ç†ï¼Œä½¿ç”¨ {len(model_files)} ä¸ªæ¨¡å‹...")
    print(f"ğŸ“Œ ä½¿ç”¨é˜ˆå€¼: {best_threshold:.4f}")

    test_transform = build_test_transform()
    dataset = MedicalDataset(config.UNLABELED_TEST_DIR, mode='test', transform=test_transform)
    loader = DataLoader(dataset, batch_size=config.BATCH_SIZE * 2, shuffle=False, num_workers=4)

    # åŠ è½½æ¨¡å‹
    models = []
    for path in model_files:
        print(f"  -> Loading {os.path.basename(path)}")
        m = get_model(config.MODEL_NAME, num_classes=config.NUM_CLASSES, pretrained=False)
        try:
            state_dict = torch.load(path, map_location=config.DEVICE, weights_only=True)
        except TypeError:
            state_dict = torch.load(path, map_location=config.DEVICE)
        
        if isinstance(state_dict, dict) and state_dict and next(iter(state_dict.keys())).startswith('module.'):
            state_dict = {k[7:]: v for k, v in state_dict.items()}
        m.load_state_dict(state_dict)
        m.to(config.DEVICE)
        m.eval()
        models.append(m)

    # æ¨ç†
    predictions = []
    print("\nRunning inference on unlabeled test set (2x TTA: identity + hflip)...")
    with torch.no_grad():
        for images, filenames in tqdm(loader):
            images = images.to(config.DEVICE)

            prob_sum = torch.zeros(images.size(0), device=config.DEVICE)
            tta_views = tta_2x(images)
            denom = float(len(models) * len(tta_views))

            for model in models:
                for view in tta_views:
                    logits = model(view)
                    prob_sum += torch.sigmoid(logits)

            avg_prob = (prob_sum / denom).detach().cpu().numpy()
            final_preds = (avg_prob >= best_threshold).astype(np.int64)

            for fname, label in zip(filenames, final_preds):
                predictions.append({"id": fname, "label": int(label)})

    # ä¿å­˜
    submission_df = pd.DataFrame(predictions)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    submission_filename = f"submission_{timestamp}.csv"
    submission_path = os.path.join(config.OUTPUT_DIR, submission_filename)

    os.makedirs(config.OUTPUT_DIR, exist_ok=True)
    submission_df.to_csv(submission_path, index=False)
    
    print(f"\nğŸ‰ Submission file created: {submission_path}")

if __name__ == '__main__':
    main()