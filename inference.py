# inference.py (æœ€ç»ˆæäº¤è„šæœ¬)
"""
åŠŸèƒ½ï¼šä½¿ç”¨å¤šæ¨¡å‹ 8x TTA å¯¹æ— æ ‡ç­¾æµ‹è¯•é›†æ¨ç†ï¼ŒæŒ‰ benchmark JSON é˜ˆå€¼ç”Ÿæˆæäº¤æ–‡ä»¶ã€‚
ç”¨æ³•ç¤ºä¾‹ï¼š
    CONFIG_INIT_DIRS=0 python inference.py \
        --model_paths "models/run_xxx/vit_fold*.pth" \
        --benchmark_json output/benchmark_result_YYYYMMDD_HHMMSS.json
è¾“å‡ºï¼š
    output/submission_YYYYMMDD_HHMMSS.csv
ä¾èµ–ï¼šbenchmark JSON æä¾›æœ€ä½³é˜ˆå€¼ï¼›æµ‹è¯•é›†è·¯å¾„ config.UNLABELED_TEST_DIRã€‚
"""

import os
import argparse
import json
import torch
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader
from tqdm import tqdm
import glob
from datetime import datetime

# åœ¨å¯¼å…¥ config ä¹‹å‰å…³é—­ç›®å½•åˆå§‹åŒ–å¼€å…³
os.environ['CONFIG_INIT_DIRS'] = '0'
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
import config
from model import get_model
from dataset import MedicalDataset, val_transform_alb

def parse_args():
    parser = argparse.ArgumentParser(description="Final Submission Inference Script")
    
    # æ¥æ”¶æ¨¡å‹è·¯å¾„ï¼Œæ”¯æŒé€šé…ç¬¦
    parser.add_argument('--model_paths', nargs='+', required=True, 
                        help="Path to model files. Use quotes for wildcards, e.g., 'models/run_xxx/vit_fold*.pth'")
    
    # æ¥æ”¶ benchmark.json æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºè‡ªåŠ¨è¯»å–æœ€ä½³é˜ˆå€¼
    parser.add_argument('--benchmark_json', type=str, required=True,
                        help="Path to the benchmark_result_xxx.json file to get the best threshold.")
                        
    return parser.parse_args()

def main():
    args = parse_args()
    
    # --- 1. æ™ºèƒ½å¤„ç†æ¨¡å‹è·¯å¾„ ---
    model_files = []
    for path_pattern in args.model_paths:
        model_files.extend(glob.glob(path_pattern))
    
    if not model_files:
        print(f"Error: No model files found matching pattern: {args.model_paths}")
        return
    print(f"Found {len(model_files)} models for inference.")

    # --- 2. ä» JSON æ–‡ä»¶ä¸­è‡ªåŠ¨è¯»å–æœ€ä½³é˜ˆå€¼ ---
    try:
        with open(args.benchmark_json, 'r') as f:
            benchmark_data = json.load(f)
        best_threshold = benchmark_data['threshold']
        print(f"Successfully loaded best threshold: {best_threshold:.4f} from {args.benchmark_json}")
    except Exception as e:
        print(f"Error loading benchmark JSON file: {e}. Please check the path.")
        return

    # --- 3. å‡†å¤‡æ— æ ‡ç­¾æµ‹è¯•é›† ---
    # ä½¿ç”¨ä¸éªŒè¯é›†å®Œå…¨ç›¸åŒçš„é¢„å¤„ç†
    test_transform = val_transform_alb 
    # ã€å…³é”®ã€‘è¯»å–æ— æ ‡ç­¾æµ‹è¯•é›†ï¼Œå¹¶å°† mode è®¾ä¸º 'test'
    dataset = MedicalDataset(config.UNLABELED_TEST_DIR, mode='test', transform=test_transform)
    # Batch size å¯ä»¥è®¾å¤§ä¸€ç‚¹åŠ é€Ÿæ¨ç†
    loader = DataLoader(dataset, batch_size=config.BATCH_SIZE * 2, shuffle=False, num_workers=4)

    # --- 4. åŠ è½½æ¨¡å‹ ---
    models = []
    for path in model_files:
        print(f"  -> Loading {os.path.basename(path)}")
        m = get_model(config.MODEL_NAME, num_classes=config.NUM_CLASSES, pretrained=False)
        state_dict = torch.load(path, map_location=config.DEVICE, weights_only=True)
        if list(state_dict.keys())[0].startswith('module.'):
            new_state_dict = {k[7:]: v for k, v in state_dict.items()}
            state_dict = new_state_dict
        m.load_state_dict(state_dict)
        m.to(config.DEVICE)
        m.eval()
        models.append(m)

    # --- 5. æ‰§è¡Œæ¨ç† (8x TTA) ---
    predictions = []
    
    print("\nRunning Inference on unlabeled test set with 8x TTA...")
    with torch.no_grad():
        # ã€å…³é”®ã€‘loader ç°åœ¨è¿”å› (images, filenames)
        for images, filenames in tqdm(loader):
            images = images.to(config.DEVICE)
            batch_probs = torch.zeros(images.size(0), 2).to(config.DEVICE)
            rotations = [0, 1, 2, 3]
            
            for model in models:
                for k in rotations:
                    img_rot = torch.rot90(images, k=k, dims=[2, 3])
                    logits = model(img_rot)
                    batch_probs += torch.softmax(logits, dim=1)
                    
                    img_rot_flip = torch.flip(img_rot, dims=[3])
                    logits_flip = model(img_rot_flip)
                    batch_probs += torch.softmax(logits_flip, dim=1)
            
            batch_probs /= (len(models) * 8)
            
            # æ ¹æ®æœ€ä½³é˜ˆå€¼ç”Ÿæˆ 0/1 æ ‡ç­¾
            final_preds = (batch_probs[:, 1] > best_threshold).int().cpu().numpy()
            
            # è®°å½•æ–‡ä»¶åå’Œå¯¹åº”çš„é¢„æµ‹æ ‡ç­¾
            for fname, label in zip(filenames, final_preds):
                predictions.append({"id": fname, "label": label})

    # --- 6. ç”Ÿæˆ submission.csv æ–‡ä»¶ ---
    if not predictions:
        print("No predictions were generated. Check your test set directory.")
        return

    # åˆ›å»º DataFrame
    submission_df = pd.DataFrame(predictions)
    
    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    submission_filename = f"submission_{timestamp}.csv"
    submission_path = os.path.join(config.OUTPUT_DIR, submission_filename)
    
    # ä¿å­˜ä¸º CSV
    submission_df.to_csv(submission_path, index=False)
    
    print(f"\nğŸ‰ Submission file created successfully!")
    print(f"   Total predictions: {len(submission_df)}")
    print(f"   Saved to: {submission_path}")

if __name__ == '__main__':
    main()