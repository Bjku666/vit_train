"""inference.py
åŠŸèƒ½å‡çº§ç‰ˆï¼š
1. æ”¯æŒ --only_eval å‚æ•°ï¼šä»…è¯»å– OOF è®¡ç®—æœ€ä½³é˜ˆå€¼å¹¶æ‰“å°ï¼Œä¸è¿›è¡Œåç»­æ¨ç†ï¼ˆç§’çº§åé¦ˆï¼‰ã€‚
2. æ¨ç†ä½¿ç”¨ä¸¥æ ¼ 2x TTAï¼šä»…åŸå›¾ + æ°´å¹³ç¿»è½¬ï¼ˆä¸¥ç¦ 90Â° æ—‹è½¬ï¼‰ã€‚
"""

import os
import argparse
import torch
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader
from tqdm import tqdm
import glob
from datetime import datetime

# åœ¨å¯¼å…¥ config ä¹‹å‰å…³é—­ç›®å½•åˆå§‹åŒ–å¼€å…³
os.environ['CONFIG_INIT_DIRS'] = '0'
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
import config
from model import get_model
from dataset import MedicalDataset

import albumentations as A
from albumentations.pytorch import ToTensorV2

def parse_args():
    parser = argparse.ArgumentParser(description="Final Submission Inference Script")
    
    # æ¥æ”¶æ¨¡å‹è·¯å¾„
    parser.add_argument('--model_paths', nargs='+', required=True, 
                        help="Path to model files. Use quotes for wildcards.")

    # OOF æ–‡ä»¶è·¯å¾„ (é»˜è®¤è¯»å– output/oof/oof_fold_*.csv)
    parser.add_argument('--oof_paths', nargs='+', default=[],
                        help="OOF csv paths (supports wildcards).")
    
    # [å…³é”®] ä»…è¯„æµ‹æ¨¡å¼å¼€å…³
    parser.add_argument('--only_eval', action='store_true', 
                        help="If set, only search best threshold based on OOF and exit.")
                        
    return parser.parse_args()


def build_test_transform():
    return A.Compose([
        A.Normalize(mean=config.IMG_MEAN, std=config.IMG_STD),
        ToTensorV2(),
    ])


def load_oof(oof_paths):
    """è¯»å–å¹¶åˆå¹¶ OOF CSV"""
    probs = []
    targets = []
    if not oof_paths:
        return np.array([]), np.array([])
        
    for p in oof_paths:
        df = pd.read_csv(p)
        if 'Preds' not in df.columns or 'Targets' not in df.columns:
            raise ValueError(f"OOF æ–‡ä»¶åˆ—åå¿…é¡»åŒ…å« Preds/Targetsï¼Œä½†åœ¨ {p} ä¸­æœªæ‰¾åˆ°")
        probs.append(df['Preds'].values.astype(np.float32))
        targets.append(df['Targets'].values.astype(np.int64))
    
    if not probs:
        return np.array([]), np.array([])
        
    probs = np.concatenate(probs, axis=0)
    targets = np.concatenate(targets, axis=0)
    return probs, targets


def search_best_threshold(probs: np.ndarray, targets: np.ndarray) -> float:
    """åœ¨ [0.2, 0.8] æœç´¢æœ€ä½³é˜ˆå€¼"""
    thresholds = np.linspace(0.2, 0.8, 601, dtype=np.float32)
    best_t = 0.5
    best_acc = -1.0
    
    # ç®€å•çš„å‘é‡åŒ–è®¡ç®—åŠ é€Ÿ
    for t in thresholds:
        preds = (probs >= t).astype(np.int64)
        acc = (preds == targets).mean()
        if acc > best_acc or (acc == best_acc and t < best_t):
            best_acc = acc
            best_t = float(t)
    
    print("\n" + "="*45)
    print(f" ğŸ“Š [OOF Evaluation] å†…éƒ¨éªŒè¯é›†è¯„æµ‹æŠ¥å‘Š")
    print(f" ---------------------------------------------")
    print(f" æ ·æœ¬æ€»æ•°: {len(targets)}")
    print(f" æœ€ä½³é˜ˆå€¼: {best_t:.4f}")
    print(f" æœ€ä½³ Acc: {best_acc:.6f}  (è¿™æ˜¯é¢„æœŸçš„ä¸Šçº¿)")
    print("="*45 + "\n")
    return best_t


def _load_solid_threshold():
    """ä» output/best_threshold.json è¯»å–å›ºåŒ–é˜ˆå€¼ï¼ˆç”±è®­ç»ƒ/è¯„æµ‹è„šæœ¬å†™å…¥ï¼‰ã€‚"""
    path = os.path.join(config.OUTPUT_DIR, 'best_threshold.json')
    if not os.path.exists(path):
        return None
    try:
        df = pd.read_json(path)
        # å…¼å®¹ dict/json
        if isinstance(df, pd.Series):
            t = float(df.get('threshold', np.nan))
        else:
            # ä¸å¤ªå¯èƒ½èµ°åˆ°è¿™é‡Œ
            t = float(df['threshold'].iloc[0])
        if np.isfinite(t):
            return t
    except Exception:
        pass
    try:
        import json
        with open(path, 'r', encoding='utf-8') as f:
            obj = json.load(f)
        t = float(obj.get('threshold', float('nan')))
        return t if np.isfinite(t) else None
    except Exception:
        return None


def _save_solid_threshold(best_threshold: float, oof_files: list[str]) -> None:
    """å°† OOF æœå‡ºæ¥çš„é˜ˆå€¼å›ºåŒ–åˆ° output/best_threshold.jsonï¼Œä¾›åç»­æ¨ç†ç›´æ¥å¤ç”¨ã€‚"""
    try:
        import json
        os.makedirs(config.OUTPUT_DIR, exist_ok=True)
        payload = {
            'run_id': config.RUN_ID,
            'stage': int(getattr(config, 'CURRENT_STAGE', 0)),
            'threshold': float(best_threshold),
            'oof_files': [os.path.basename(p) for p in oof_files],
        }
        path_latest = os.path.join(config.OUTPUT_DIR, 'best_threshold.json')
        with open(path_latest, 'w', encoding='utf-8') as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)
    except Exception:
        return


def tta_2x(images: torch.Tensor):
    """ä¸¥æ ¼ 2x TTAï¼šä»…åŸå›¾ + æ°´å¹³ç¿»è½¬ï¼ˆä¸åšä»»ä½•æ—‹è½¬ï¼‰"""
    return [images, torch.flip(images, dims=[3])]


def tta_shift(images: torch.Tensor, shift_px: int):
    pad = torch.nn.functional.pad(images, (0, 0, 0, 0, shift_px, shift_px, shift_px, shift_px))
    views = []
    views.append(pad[:, :, shift_px*2:, shift_px:-shift_px])  # up
    views.append(pad[:, :, :-shift_px*2, shift_px:-shift_px])  # down
    views.append(pad[:, :, shift_px:-shift_px, shift_px*2:])  # left
    views.append(pad[:, :, shift_px:-shift_px, :-shift_px*2])  # right
    return views

def main():
    args = parse_args()
    
    # --- 1. ç¡®å®š OOF æ–‡ä»¶ ---
    oof_files = []
    if args.oof_paths:
        for p in args.oof_paths:
            oof_files.extend(glob.glob(p))
    else:
        # é»˜è®¤å» config ç›®å½•æ‰¾
        oof_files.extend(glob.glob(os.path.join(config.OOF_DIR, 'oof_fold_*.csv')))

    oof_files = sorted(oof_files)
    best_threshold = None
    if not args.only_eval:
        best_threshold = _load_solid_threshold()
        if best_threshold is not None:
            print(f"[Info] ä½¿ç”¨å›ºåŒ–é˜ˆå€¼ output/best_threshold.json: {best_threshold:.4f}")

    if best_threshold is None:
        if not oof_files:
            print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ° OOF æ–‡ä»¶ï¼Œå°†ä½¿ç”¨é»˜è®¤é˜ˆå€¼ 0.5ã€‚è¯·æ£€æŸ¥ {config.OOF_DIR}")
            best_threshold = 0.5
        else:
            print(f"[Info] åŠ è½½ {len(oof_files)} ä¸ª OOF æ–‡ä»¶è¿›è¡Œé˜ˆå€¼æœç´¢...")
            probs, targets = load_oof(oof_files)
            if len(targets) == 0:
                print("âš ï¸  OOFæ–‡ä»¶ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤é˜ˆå€¼0.5")
                best_threshold = 0.5
            else:
                best_threshold = search_best_threshold(probs, targets)
                if not args.only_eval:
                    _save_solid_threshold(best_threshold, oof_files)

    # === [æ ¸å¿ƒé€»è¾‘] å¦‚æœåªæ˜¯è¯„æµ‹ï¼Œåˆ°è¿™é‡Œå°±ç»“æŸ ===
    if args.only_eval:
        print("âœ… è¯„æµ‹å®Œæˆ (--only_eval)ã€‚ä¸æ‰§è¡Œæ¨ç†æäº¤ã€‚")
        return

    # --- 2. æ­£å¼æ¨ç†æµç¨‹ ---
    model_files = []
    for path_pattern in args.model_paths:
        model_files.extend(glob.glob(path_pattern))

    if not model_files:
        print(f"âŒ Error: No model files found matching: {args.model_paths}")
        return
    
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ¨ç†ï¼Œä½¿ç”¨ {len(model_files)} ä¸ªæ¨¡å‹...")
    print(f"ğŸ“Œ ä½¿ç”¨é˜ˆå€¼: {best_threshold:.4f}")

    test_transform = build_test_transform()
    dataset = MedicalDataset(config.UNLABELED_TEST_DIR, mode='test', transform=test_transform)
    loader = DataLoader(dataset, batch_size=config.BATCH_SIZE * 2, shuffle=False, num_workers=4)

    # åŠ è½½æ¨¡å‹
    models = []
    for path in model_files:
        print(f"  -> Loading {os.path.basename(path)}")
        m = get_model(config.MODEL_NAME, num_classes=config.NUM_CLASSES, pretrained=False)
        try:
            state_dict = torch.load(path, map_location=config.DEVICE, weights_only=True)
        except TypeError:
            state_dict = torch.load(path, map_location=config.DEVICE)
        
        if isinstance(state_dict, dict) and state_dict and next(iter(state_dict.keys())).startswith('module.'):
            state_dict = {k[7:]: v for k, v in state_dict.items()}
        m.load_state_dict(state_dict)
        m.to(config.DEVICE)
        m.eval()
        models.append(m)

    shift_tta = int(os.environ.get('SHIFT_TTA', '0')) == 1
    shift_px = int(os.environ.get('SHIFT_PX', '8'))

    # æ¨ç†
    predictions = []
    print("\nRunning inference on unlabeled test set (TTA: identity + hflip{} )...".format(" + shift" if shift_tta else ""))
    with torch.no_grad():
        for images, filenames in tqdm(loader):
            images = images.to(config.DEVICE)

            prob_sum = torch.zeros(images.size(0), device=config.DEVICE)
            tta_views = tta_2x(images)
            if shift_tta:
                tta_views.extend(tta_shift(images, shift_px))
                tta_views.extend([torch.flip(v, dims=[3]) for v in tta_views if v is not images])
            denom = float(len(models) * len(tta_views))

            for model in models:
                for view in tta_views:
                    logits = model(view)
                    prob_sum += torch.sigmoid(logits)

            avg_prob = (prob_sum / denom).detach().cpu().numpy()
            final_preds = (avg_prob >= best_threshold).astype(np.int64)

            for fname, label in zip(filenames, final_preds):
                predictions.append({"id": fname, "label": int(label)})

    # ä¿å­˜
    submission_df = pd.DataFrame(predictions)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    submission_filename = f"submission_{timestamp}.csv"
    submission_path = os.path.join(config.OUTPUT_DIR, submission_filename)

    os.makedirs(config.OUTPUT_DIR, exist_ok=True)
    submission_df.to_csv(submission_path, index=False)
    
    print(f"\nğŸ‰ Submission file created: {submission_path}")

if __name__ == '__main__':
    main()